# -*- coding: utf-8 -*-
"""Model Sistem Rekomendasi dengan Kerangka Evaluasi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vxk3BtK_i4f-Y__hp443IBvCvtOB6_BX
"""

import pandas as pd
import numpy as np
from collections import defaultdict
from surprise import Dataset, Reader, KNNWithMeans
from sklearn.preprocessing import minmax_scale
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# =============================================================================
# 1. DATA SIMULATION & PREPARATION
# =============================================================================
def create_mock_dataset(num_users, num_items, num_interactions):
    """Creates a mock dataset based on the specified characteristics."""
    print("Creating mock dataset...")
    users = np.random.randint(0, num_users, num_interactions)
    items = np.random.randint(0, num_items, num_interactions)
    ratings = np.random.randint(1, 6, num_interactions)

    df = pd.DataFrame({
        'user_id': users,
        'item_id': items,
        'rating': ratings
    })
    df = df.drop_duplicates(subset=['user_id', 'item_id']).reset_index(drop=True)
    print(f"Mock dataset created with {len(df)} unique interactions.")
    return df

def split_data(df):
    """Splits the dataframe into training and testing sets."""
    print("Splitting data into training and test sets...")
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    print(f"Train set size: {len(train_df)}, Test set size: {len(test_df)}")
    return train_df, test_df

def simulate_context_data(item_ids):
    """
    Simulates weather and traffic data for a list of item IDs.
    """
    if not isinstance(item_ids, list) or not item_ids:
        return pd.DataFrame(columns=['item_id', 'weather_suitability', 'traffic_condition'])
    n_items = len(item_ids)
    context_df = pd.DataFrame({
        'item_id': item_ids,
        'weather_suitability': np.random.rand(n_items),
        'traffic_condition': np.random.rand(n_items)
    })
    return context_df

# =============================================================================
# 2. BASE CLASS FOR RECOMMENDERS
# =============================================================================
class BaseRecommender:
    """Base class for all recommender models to ensure a consistent interface."""
    def fit(self, ratings_df, **kwargs):
        raise NotImplementedError
    def predict(self, user_id, n=10, return_scores=False, items_to_ignore=[], **kwargs):
        raise NotImplementedError

# =============================================================================
# 3. MODEL IMPLEMENTATIONS
# =============================================================================
class PopularityRecommender(BaseRecommender):
    def __init__(self):
        self.popular_items = None
        self.name = "Most Popular"
    def fit(self, ratings_df, **kwargs):
        item_stats = ratings_df.groupby('item_id')['rating'].agg(['mean', 'count']).reset_index()
        self.popular_items = item_stats.sort_values(by=['mean', 'count'], ascending=False)
    def predict(self, user_id, n=10, return_scores=False, items_to_ignore=[], **kwargs):
        if self.popular_items is None: raise RuntimeError("Model not fitted.")
        recs = self.popular_items[~self.popular_items['item_id'].isin(items_to_ignore)]
        top_n_df = recs.head(n)
        return top_n_df[['item_id', 'mean']].rename(columns={'mean': 'score'}) if return_scores else top_n_df['item_id'].tolist()

class UserBasedCFRecommender(BaseRecommender):
    def __init__(self, k=40, sim_options={'name': 'cosine', 'user_based': True}):
        self.algo = KNNWithMeans(k=k, sim_options=sim_options, verbose=False)
        self.trainset = None; self.all_items = None; self.name = "User-Based CF"
    def fit(self, ratings_df, **kwargs):
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(ratings_df[['user_id', 'item_id', 'rating']], reader)
        self.trainset = data.build_full_trainset()
        self.algo.fit(self.trainset)
        self.all_items = list(set(ratings_df['item_id']))
    def predict(self, user_id, n=10, return_scores=False, items_to_ignore=[], **kwargs):
        if self.trainset is None: raise RuntimeError("Model not fitted.")
        try: inner_user_id = self.trainset.to_inner_uid(user_id)
        except ValueError: return pd.DataFrame({'item_id': [], 'score': []}) if return_scores else []
        rated_items_inner_ids = {item_id for (item_id, _) in self.trainset.ur[inner_user_id]}
        items_to_predict = [item_id for item_id in self.all_items if self.trainset.to_inner_iid(item_id) not in rated_items_inner_ids and item_id not in items_to_ignore]
        predictions = [self.algo.predict(uid=user_id, iid=item_id) for item_id in items_to_predict]
        predictions.sort(key=lambda x: x.est, reverse=True)
        top_n_preds = predictions[:n]
        top_n_df = pd.DataFrame([(pred.iid, pred.est) for pred in top_n_preds], columns=['item_id', 'score'])
        return top_n_df if return_scores else top_n_df['item_id'].tolist()

class HybridStaticRecommender(BaseRecommender):
    def __init__(self, recommender1, recommender2, weight1=0.7, weight2=0.3):
        self.rec1, self.rec2, self.w1, self.w2, self.name = recommender1, recommender2, weight1, weight2, "Hybrid-Static"
    def fit(self, ratings_df, **kwargs):
        self.rec1.fit(ratings_df); self.rec2.fit(ratings_df)
    def predict(self, user_id, n=10, return_scores=False, items_to_ignore=[], **kwargs):
        preds1_df = self.rec1.predict(user_id, n=200, return_scores=True, items_to_ignore=items_to_ignore)
        preds2_df = self.rec2.predict(user_id, n=200, return_scores=True, items_to_ignore=items_to_ignore)
        if preds1_df.empty: return preds2_df.head(n)[['item_id', 'score']] if return_scores else preds2_df.head(n)['item_id'].tolist()
        merged_df = pd.merge(preds1_df, preds2_df, on='item_id', how='outer').fillna(0)
        merged_df['score_x_norm'] = minmax_scale(merged_df['score_x'])
        merged_df['score_y_norm'] = minmax_scale(merged_df['score_y'])
        merged_df['hybrid_score'] = (self.w1 * merged_df['score_x_norm']) + (self.w2 * merged_df['score_y_norm'])
        top_n_df = merged_df.sort_values(by='hybrid_score', ascending=False).head(n)
        return top_n_df[['item_id', 'hybrid_score']].rename(columns={'hybrid_score':'score'}) if return_scores else top_n_df['item_id'].tolist()

# =============================================================================
# 4. Hybrid-Adaptive Recommender (Proposed Model with Multi-stage Re-ranking)
# =============================================================================
class HybridAdaptiveRecommender(BaseRecommender):
    def __init__(self, recommender1, recommender2, weight1=0.7, weight2=0.3,
                 bias_correction_strength=0.1,
                 context_weights={'weather': 0.25, 'traffic': 0.15},
                 mmr_lambda=0.8): # *** UPDATED: Increased lambda to prioritize accuracy ***
        self.candidate_generator = HybridStaticRecommender(recommender1, recommender2, weight1, weight2)
        self.bias_strength = bias_correction_strength
        self.context_weights = context_weights
        self.mmr_lambda = mmr_lambda # Controls trade-off: 1.0=Accuracy, 0.0=Diversity
        self.rec_weight = 1.0 - sum(self.context_weights.values())
        self.name = "Hybrid-Adaptive"
        self.item_popularity = None

    def fit(self, ratings_df, **kwargs):
        print(f"Fitting {self.name}...")
        self.candidate_generator.fit(ratings_df)
        # Calculate item popularity for bias correction
        self.item_popularity = ratings_df['item_id'].value_counts(normalize=True)

    def _maximal_marginal_relevance(self, candidates_df, similarity_matrix, n):
        candidates_df = candidates_df.set_index('item_id')
        item_scores = candidates_df['final_score'].to_dict()

        selected_items = []
        candidate_items = list(candidates_df.index)

        if not candidate_items: return []
        best_initial_item = max(candidate_items, key=lambda item: item_scores.get(item, 0))
        selected_items.append(best_initial_item)
        candidate_items.remove(best_initial_item)

        while len(selected_items) < n and candidate_items:
            best_next_item = -1
            max_mmr_score = -np.inf

            for item in candidate_items:
                relevance = item_scores.get(item, 0)
                max_sim_with_selected = 0
                if selected_items:
                     max_sim_with_selected = max([similarity_matrix.loc[item, sel_item] for sel_item in selected_items if sel_item in similarity_matrix.columns and item in similarity_matrix.index])

                # The core MMR calculation
                mmr_score = self.mmr_lambda * relevance - (1 - self.mmr_lambda) * max_sim_with_selected

                if mmr_score > max_mmr_score:
                    max_mmr_score = mmr_score
                    best_next_item = item

            if best_next_item != -1:
                selected_items.append(best_next_item)
                candidate_items.remove(best_next_item)

        return selected_items

    def predict(self, user_id, n=10, return_scores=False, items_to_ignore=[], **kwargs):
        similarity_matrix = kwargs.get('similarity_matrix')
        if similarity_matrix is None:
            raise ValueError("Similarity matrix must be provided for MMR.")

        # Stage 1: Candidate Generation
        candidate_df = self.candidate_generator.predict(user_id, n=100, return_scores=True, items_to_ignore=items_to_ignore)
        if candidate_df.empty: return [] if not return_scores else pd.DataFrame(columns=['item_id', 'score'])

        # Stage 2: Multi-stage Re-ranking
        # a) Bias Correction
        candidate_df['popularity'] = candidate_df['item_id'].map(self.item_popularity).fillna(0)
        candidate_df['score_corrected'] = candidate_df['score'] * (1 - self.bias_strength * candidate_df['popularity'])

        # b) Contextual Adjustment
        context_df = simulate_context_data(candidate_df['item_id'].tolist())
        re_ranked_df = pd.merge(candidate_df, context_df, on='item_id')

        re_ranked_df['rec_score_norm'] = minmax_scale(re_ranked_df['score_corrected'])
        re_ranked_df['final_score'] = (self.rec_weight * re_ranked_df['rec_score_norm']) + \
                                      (self.context_weights['weather'] * re_ranked_df['weather_suitability']) + \
                                      (self.context_weights['traffic'] * re_ranked_df['traffic_condition'])

        # c) Diversity Enhancement (MMR)
        final_recommendations = self._maximal_marginal_relevance(re_ranked_df, similarity_matrix, n)

        if return_scores:
             final_df_scores = re_ranked_df[re_ranked_df['item_id'].isin(final_recommendations)][['item_id', 'final_score']].rename(columns={'final_score':'score'})
             return final_df_scores.set_index('item_id').loc[final_recommendations].reset_index()

        return final_recommendations

# =============================================================================
# 5. EVALUATION FRAMEWORK
# =============================================================================
class Evaluator:
    def __init__(self, train_df, test_df):
        self.train_df, self.test_df, self.test_users = train_df, test_df, test_df['user_id'].unique()
        self.all_items = train_df['item_id'].unique()
        self.ground_truth = self.test_df[self.test_df['rating'] >= 4].groupby('user_id')['item_id'].apply(list).to_dict()
        self.item_popularity = self.train_df['item_id'].value_counts(normalize=True)
        self.similarity_matrix = self._calculate_similarity_matrix()
    def _calculate_similarity_matrix(self):
        print("Calculating item-item similarity matrix for diversity metric...")
        item_user_matrix = self.train_df.pivot_table(index='item_id', columns='user_id', values='rating').fillna(0)
        item_user_matrix = item_user_matrix.reindex(index=sorted(item_user_matrix.index), columns=sorted(item_user_matrix.columns))
        similarity = cosine_similarity(item_user_matrix)
        sim_df = pd.DataFrame(similarity, index=item_user_matrix.index, columns=item_user_matrix.index)
        print("Similarity matrix calculated.")
        return sim_df
    def get_ground_truth_for_user(self, user_id): return self.ground_truth.get(user_id, [])
    def calculate_precision_recall_f1(self, recommendations, ground_truth, k):
        if not ground_truth: return 0, 0, 0
        rec_set, gt_set = set(recommendations[:k]), set(ground_truth)
        true_positives = len(rec_set.intersection(gt_set))
        precision = true_positives / k if k > 0 else 0
        recall = true_positives / len(gt_set) if len(gt_set) > 0 else 0
        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        return precision, recall, f1
    def calculate_intra_list_diversity(self, recommendations):
        if len(recommendations) < 2: return 0.0
        total_dissimilarity, num_pairs = 0, 0
        for i in range(len(recommendations)):
            for j in range(i + 1, len(recommendations)):
                item1, item2 = recommendations[i], recommendations[j]
                if item1 in self.similarity_matrix and item2 in self.similarity_matrix.columns:
                    total_dissimilarity += (1 - self.similarity_matrix.loc[item1, item2])
                    num_pairs += 1
        return total_dissimilarity / num_pairs if num_pairs > 0 else 0
    def calculate_novelty(self, recommendations):
        if not recommendations: return 0
        return -np.sum([np.log2(self.item_popularity.get(item, 1e-9)) for item in recommendations]) / len(recommendations)
    def evaluate_model(self, model, k=5):
        print(f"\n--- Evaluating model: {model.name} ---")
        model.fit(self.train_df)
        precisions, recalls, f1s, diversities, novelties = [], [], [], [], []
        all_recommendations = set()
        train_user_items = self.train_df.groupby('user_id')['item_id'].apply(list).to_dict()
        for user_id in self.test_users:
            items_to_ignore = train_user_items.get(user_id, [])
            predict_kwargs = {'similarity_matrix': self.similarity_matrix} if isinstance(model, HybridAdaptiveRecommender) else {}
            recommendations = model.predict(user_id, n=k, items_to_ignore=items_to_ignore, **predict_kwargs)
            ground_truth = self.get_ground_truth_for_user(user_id)
            if not recommendations: continue
            all_recommendations.update(recommendations)
            precision, recall, f1 = self.calculate_precision_recall_f1(recommendations, ground_truth, k)
            precisions.append(precision); recalls.append(recall); f1s.append(f1)
            diversities.append(self.calculate_intra_list_diversity(recommendations))
            novelties.append(self.calculate_novelty(recommendations))
        coverage = len(all_recommendations) / len(self.all_items) if self.all_items.size > 0 else 0
        return {f"Precision@{k}": np.mean(precisions), f"Recall@{k}": np.mean(recalls), f"F1-Score@{k}": np.mean(f1s), "Intra-List Diversity": np.mean(diversities), "Novelty": np.mean(novelties), "Catalog Coverage": coverage}

# =============================================================================
# 6. EXPERIMENT EXECUTION & VISUALIZATION
# =============================================================================
def visualize_results(results_df):
    """Plots F1-Score vs Intra-List Diversity."""
    print("\nGenerating visualization...")
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(10, 6))

    colors = plt.cm.viridis(np.linspace(0, 1, len(results_df)))

    ax.scatter(results_df['Intra-List Diversity'], results_df['F1-Score@5'], s=150, c=colors, alpha=0.7, edgecolors='w')

    for i, model_name in enumerate(results_df.index):
        ax.text(results_df['Intra-List Diversity'][i] + 0.001, results_df['F1-Score@5'][i], model_name, fontsize=11)

    ax.set_title('Trade-off Akurasi vs Keberagaman (F1-Score vs Diversity)', fontsize=16, pad=20)
    ax.set_xlabel('Intra-List Diversity (Keberagaman)', fontsize=12)
    ax.set_ylabel('F1-Score@5 (Akurasi)', fontsize=12)

    plt.xlim(left=max(0, results_df['Intra-List Diversity'].min() - 0.02), right=results_df['Intra-List Diversity'].max() + 0.02)
    plt.ylim(bottom=max(0, results_df['F1-Score@5'].min() - 0.01), top=results_df['F1-Score@5'].max() + 0.01)

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    NUM_USERS, NUM_ITEMS, NUM_INTERACTIONS = 1000, 100, 10000
    ratings_data = create_mock_dataset(NUM_USERS, NUM_ITEMS, NUM_INTERACTIONS)
    train_data, test_data = split_data(ratings_data)

    evaluator = Evaluator(train_data, test_data)

    pop_model = PopularityRecommender()
    ubcf_model = UserBasedCFRecommender()
    hybrid_static_model = HybridStaticRecommender(ubcf_model, pop_model, weight1=0.7)
    # *** UPDATED: Pass the new lambda value during initialization ***
    hybrid_adaptive_model = HybridAdaptiveRecommender(ubcf_model, pop_model, weight1=0.7, mmr_lambda=0.8)

    models_to_evaluate = [pop_model, ubcf_model, hybrid_static_model, hybrid_adaptive_model]

    all_results = []
    for model in models_to_evaluate:
        results = evaluator.evaluate_model(model, k=5)
        results['Model'] = model.name
        all_results.append(results)

    results_df = pd.DataFrame(all_results).set_index('Model')
    print("\n\n" + "="*70)
    print("                    EVALUATION RESULTS (k=5)")
    print("="*70)
    print(results_df.to_string(float_format="%.4f"))
    print("="*70)

    visualize_results(results_df)